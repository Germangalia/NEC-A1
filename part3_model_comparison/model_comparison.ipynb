{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model Comparison (BP, BP-F, MLR-F)\n",
    "\n",
    "In this notebook, we will compare three different models:\n",
    "1. BP: Neural Network with Back-Propagation (from Part 2, implemented from scratch)\n",
    "2. BP-F: Neural Network with Back-Propagation from a library (using scikit-learn)\n",
    "3. MLR-F: Multiple Linear Regression from scikit-learn\n",
    "\n",
    "We will evaluate these models using MSE, MAE, and MAPE metrics and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Add path to access the custom BP implementation from Part 2\n",
    "sys.path.append('../part2_bp_implementation/')\n",
    "from NeuralNet import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = \"../dataset/shopping_behavior.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "print(\"First few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for models\n",
    "# Select features and target variable\n",
    "# We'll use numerical features and one-hot encode categorical features\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target variable from features if it's in the numerical columns\n",
    "target_col = 'Purchase Amount (USD)'\n",
    "if target_col in numerical_cols:\n",
    "    numerical_cols.remove(target_col)\n",
    "\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df[numerical_cols + categorical_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_cols, prefix=categorical_cols)\n",
    "\n",
    "print(f\"Encoded features shape: {X_encoded.shape}\")\n",
    "print(f\"Encoded features columns: {X_encoded.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set - X: {X_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"Test set - X: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features have been standardized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Multiple Linear Regression (MLR-F) model\n",
    "mlr_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the MLR model...\")\n",
    "mlr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "y_train_pred_mlr = mlr_model.predict(X_train_scaled)\n",
    "y_test_pred_mlr = mlr_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"MLR model predictions completed.\")\n",
    "print(f\"Training predictions shape: {y_train_pred_mlr.shape}\")\n",
    "print(f\"Test predictions shape: {y_test_pred_mlr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Neural Network with Back-Propagation from library (BP-F) using scikit-learn\n",
    "# Using MLPRegressor which implements neural networks with backpropagation\n",
    "bp_f_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,),  # Default: single hidden layer with 100 neurons\n",
    "    activation='relu',         # Default activation function\n",
    "    solver='adam',             # Default solver\n",
    "    alpha=0.0001,              # L2 regularization parameter\n",
    "    batch_size='auto',         # Default: min(200, n_samples)\n",
    "    learning_rate='constant',  # Learning rate schedule\n",
    "    learning_rate_init=0.001,  # Initial learning rate\n",
    "    max_iter=200,              # Maximum number of iterations\n",
    "    shuffle=True,              # Shuffle samples in each iteration\n",
    "    random_state=42,           # For reproducible results\n",
    "    early_stopping=True,       # Stop when validation score stops improving\n",
    "    validation_fraction=0.1,   # Fraction of training data for validation\n",
    "    n_iter_no_change=10        # Number of iterations with no improvement to wait\n",
    ")\n",
    "\n",
    "# Train the BP-F model\n",
    "print(\"Training the BP-F model (MLP with scikit-learn)...\")\n",
    "bp_f_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "y_train_pred_bp_f = bp_f_model.predict(X_train_scaled)\n",
    "y_test_pred_bp_f = bp_f_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"BP-F model predictions completed.\")\n",
    "print(f\"Training predictions shape: {y_train_pred_bp_f.shape}\")\n",
    "print(f\"Test predictions shape: {y_test_pred_bp_f.shape}\")\n",
    "print(f\"Number of iterations: {bp_f_model.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics functions\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error\"\"\"\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_true = y_true != 0\n",
    "    # Avoid division by zero by only calculating MAPE for non-zero true values\n",
    "    mape = np.mean(np.abs((y_true[non_zero_true] - y_pred[non_zero_true]) / y_true[non_zero_true])) * 100\n",
    "    return mape\n",
    "\n",
    "# Calculate evaluation metrics for all models on both training and test sets\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "\n",
    "# MLR-F model metrics\n",
    "mlr_train_mse = calculate_mse(y_train, y_train_pred_mlr)\n",
    "mlr_test_mse = calculate_mse(y_test, y_test_pred_mlr)\n",
    "\n",
    "mlr_train_mae = calculate_mae(y_train, y_train_pred_mlr)\n",
    "mlr_test_mae = calculate_mae(y_test, y_test_pred_mlr)\n",
    "\n",
    "mlr_train_mape = calculate_mape(y_train, y_train_pred_mlr)\n",
    "mlr_test_mape = calculate_mape(y_test, y_test_pred_mlr)\n",
    "\n",
    "print(f\"MLR-F Training - MSE: {mlr_train_mse:.4f}, MAE: {mlr_train_mae:.4f}, MAPE: {mlr_train_mape:.4f}%\")\n",
    "print(f\"MLR-F Test - MSE: {mlr_test_mse:.4f}, MAE: {mlr_test_mae:.4f}, MAPE: {mlr_test_mape:.4f}%\")\n",
    "\n",
    "# BP-F model metrics\n",
    "bp_f_train_mse = calculate_mse(y_train, y_train_pred_bp_f)\n",
    "bp_f_test_mse = calculate_mse(y_test, y_test_pred_bp_f)\n",
    "\n",
    "bp_f_train_mae = calculate_mae(y_train, y_train_pred_bp_f)\n",
    "bp_f_test_mae = calculate_mae(y_test, y_test_pred_bp_f)\n",
    "\n",
    "bp_f_train_mape = calculate_mape(y_train, y_train_pred_bp_f)\n",
    "bp_f_test_mape = calculate_mape(y_test, y_test_pred_bp_f)\n",
    "\n",
    "print(f\"BP-F Training - MSE: {bp_f_train_mse:.4f}, MAE: {bp_f_train_mae:.4f}, MAPE: {bp_f_train_mape:.4f}%\")\n",
    "print(f\"BP-F Test - MSE: {bp_f_test_mse:.4f}, MAE: {bp_f_test_mae:.4f}, MAPE: {bp_f_test_mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for custom BP model (from Part 2)\n",
    "# Test at least 10 hyperparameter combinations\n",
    "\n",
    "# Prepare data for the custom BP model (reshape if needed)\n",
    "X_train_bp = X_train_scaled.astype(np.float32)\n",
    "X_test_bp = X_test_scaled.astype(np.float32)\n",
    "y_train_bp = y_train.values.astype(np.float32)\n",
    "y_test_bp = y_test.values.astype(np.float32)\n",
    "\n",
    "# Define hyperparameter combinations to test\n",
    "hyperparameter_combinations = [\n",
    "    # Combination 1\n",
    "    {\"layers\": [X_train_bp.shape[1], 10, 1], \"epochs\": 100, \"learning_rate\": 0.01, \"momentum\": 0.5, \"activation\": \"sigmoid\"},\n",
    "    # Combination 2\n",
    "    {\"layers\": [X_train_bp.shape[1], 20, 1], \"epochs\": 100, \"learning_rate\": 0.01, \"momentum\": 0.5, \"activation\": \"sigmoid\"},\n",
    "    # Combination 3\n",
    "    {\"layers\": [X_train_bp.shape[1], 10, 1], \"epochs\": 200, \"learning_rate\": 0.01, \"momentum\": 0.5, \"activation\": \"sigmoid\"},\n",
    "    # Combination 4\n",
    "    {\"layers\": [X_train_bp.shape[1], 10, 1], \"epochs\": 100, \"learning_rate\": 0.001, \"momentum\": 0.5, \"activation\": \"sigmoid\"},\n",
    "    # Combination 5\n",
    "    {\"layers\": [X_train_bp.shape[1], 10, 1], \"epochs\": 100, \"learning_rate\": 0.01, \"momentum\": 0.9, \"activation\": \"sigmoid\"},\n",
    "    # Combination 6\n",
    "    {\"layers\": [X_train_bp.shape[1], 10, 1], \"epochs\": 100, \"learning_rate\": 0.01, \"momentum\": 0.5, \"activation\": \"relu\"},\n",
    "    # Combination 7\n",
    "    {\"layers\": [X_train_bp.shape[1], 15, 1], \"epochs\": 150, \"learning_rate\": 0.005, \"momentum\": 0.7, \"activation\": \"tanh\"},\n",
    "    # Combination 8\n",
    "    {\"layers\": [X_train_bp.shape[1], 30, 15, 1], \"epochs\": 100, \"learning_rate\": 0.01, \"momentum\": 0.5, \"activation\": \"sigmoid\"},\n",
    "    # Combination 9\n",
    "    {\"layers\": [X_train_bp.shape[1], 25, 10, 1], \"epochs\": 200, \"learning_rate\": 0.001, \"momentum\": 0.8, \"activation\": \"relu\"},\n",
    "    # Combination 10\n",
    "    {\"layers\": [X_train_bp.shape[1], 20, 1], \"epochs\": 150, \"learning_rate\": 0.005, \"momentum\": 0.6, \"activation\": \"tanh\"},\n",
    "    # Combination 11 (additional)\n",
    "    {\"layers\": [X_train_bp.shape[1], 12, 6, 1], \"epochs\": 120, \"learning_rate\": 0.01, \"momentum\": 0.4, \"activation\": \"relu\"},\n",
    "    # Combination 12 (additional)\n",
    "    {\"layers\": [X_train_bp.shape[1], 8, 1], \"epochs\": 180, \"learning_rate\": 0.001, \"momentum\": 0.9, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(hyperparameter_combinations)} hyperparameter combinations for custom BP model...\")\n",
    "\n",
    "# Store the results\n",
    "results_bp = []\n",
    "\n",
    "# Test each hyperparameter combination\n",
    "for i, params in enumerate(hyperparameter_combinations):\n",
    "    print(f\"Testing combination {i+1}/{len(hyperparameter_combinations)}: Layers {params['layers']}, Epochs {params['epochs']}, LR {params['learning_rate']}, Momentum {params['momentum']}, Activation {params['activation']}\")\n",
    "    \n",
    "    try:\n",
    "        # Create and configure the neural network\n",
    "        nn = NeuralNet(\n",
    "            layers=params[\"layers\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            momentum=params[\"momentum\"],\n",
    "            fact=params[\"activation\"]\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        nn.fit(X_train_bp, y_train_bp, epochs=params[\"epochs\"], validation_split=0.2)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_train_pred = nn.predict(X_train_bp)\n",
    "        y_test_pred = nn.predict(X_test_bp)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mse = calculate_mse(y_train_bp, y_train_pred)\n",
    "        test_mse = calculate_mse(y_test_bp, y_test_pred)\n",
    "        \n",
    "        train_mae = calculate_mae(y_train_bp, y_train_pred)\n",
    "        test_mae = calculate_mae(y_test_bp, y_test_pred)\n",
    "        \n",
    "        train_mape = calculate_mape(y_train_bp, y_train_pred)\n",
    "        test_mape = calculate_mape(y_test_bp, y_test_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results_bp.append({\n",
    "            \"combination\": i+1,\n",
    "            \"layers\": params[\"layers\"],\n",
    "            \"epochs\": params[\"epochs\"],\n",
    "            \"learning_rate\": params[\"learning_rate\"],\n",
    "            \"momentum\": params[\"momentum\"],\n",
    "            \"activation\": params[\"activation\"],\n",
    "            \"train_mse\": train_mse,\n",
    "            \"test_mse\": test_mse,\n",
    "            \"train_mae\": train_mae,\n",
    "            \"test_mae\": test_mae,\n",
    "            \"train_mape\": train_mape,\n",
    "            \"test_mape\": test_mape\n",
    "        })\n",
    "        \n",
    "        print(f\"  -> Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "        print(f\"  -> Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"  -> Train MAPE: {train_mape:.4f}%, Test MAPE: {test_mape:.4f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error with combination {i+1}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nHyperparameter tuning completed. Tested {len(results_bp)} valid combinations.\")\n",
    "\n",
    "# Find the best combination based on test MSE\n",
    "if results_bp:\n",
    "    best_result = min(results_bp, key=lambda x: x['test_mse'])\n",
    "    print(f\"\\nBest combination based on test MSE:\")\n",
    "    print(f\"  Combination {best_result['combination']}: Layers {best_result['layers']}, \"\n",
    "          f\"Epochs {best_result['epochs']}, LR {best_result['learning_rate']}, \"\n",
    "          f\"Momentum {best_result['momentum']}, Activation {best_result['activation']}\")\n",
    "    print(f\"  Test MSE: {best_result['test_mse']:.4f}, Test MAE: {best_result['test_mae']:.4f}, Test MAPE: {best_result['test_mape']:.4f}%\")\n",
    "    \n",
    "    # Create a table summary of results\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame(results_bp)\n",
    "    print(\"\\nResults Summary Table:\")\n",
    "    print(results_df[[\"combination\", \"layers\", \"epochs\", \"learning_rate\", \"momentum\", \"activation\", \"test_mse\", \"test_mae\", \"test_mape\"]].round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}